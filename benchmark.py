import argparse
import csv
import json
import os
import time
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple

from rich.rule import Rule
from rich.table import Table

import src.console as console
from src.agents import CRITIQUE_CRITERIA, redis_client
from src.graph import app
from src.vector_db import ensure_collection, vector_store

CRITERIA_HEADERS: Dict[str, Tuple[str, str]] = {
    "ë…¼ë¦¬ì  ì¼ê´€ì„±": ("logical_consistency_score", "logical_consistency_reason"),
    "ë²•ë¥ ì  íƒ€ë‹¹ì„±": ("legal_validity_score", "legal_validity_reason"),
    "ì‚¬íšŒì  ê°€ì¹˜ ê³ ë ¤": ("social_consideration_score", "social_consideration_reason"),
}


def run_benchmark(test_filepath: str, is_trained: bool):
    """ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    mode = "í•™ìŠµ í›„ (Trained)" if is_trained else "í•™ìŠµ ì „ (Untrained)"
    console.print_header(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘: {mode}")

    if not is_trained:
        console.console.print("[bold yellow]ê²½ê³ : ëª¨ë“  DB(Redis, PostgreSQL)ì˜ ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.[/bold yellow]")
        redis_client.flushall()
        console.console.print("ğŸ”´ Redis DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        try:
            vector_store.delete_collection()
            ensure_collection()
            console.console.print("ğŸ”´ PostgreSQL ë²¡í„° DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            console.console.print(f"ğŸŸ¡ PostgreSQL ë²¡í„° DB ì´ˆê¸°í™” ì¤‘ ì°¸ê³ : {e}")
    else:
        console.console.print("ì‚¬ì „ í•™ìŠµëœ DBë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.")

    try:
        with open(test_filepath, "r", encoding="utf-8") as f:
            test_cases = [json.loads(line) for line in f]
    except FileNotFoundError:
        console.console.print(f"[bold red]ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {test_filepath}[/bold red]")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_filename = f"benchmark_results_{mode.replace(' ', '_')}_{timestamp}.csv"

    with open(results_filename, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        header = [
            "case_id",
            "expected_outcome",
            "model_outcome",
            "is_correct",
            "logical_consistency_score",
            "logical_consistency_reason",
            "legal_validity_score",
            "legal_validity_reason",
            "social_consideration_score",
            "social_consideration_reason",
        ]
        writer.writerow(header)

        total_scores = defaultdict(float)
        total_runs = 0
        paired_outcomes: List[Tuple[str, str]] = []

        for i, case in enumerate(test_cases):
            case_id = case.get("caseId", "N/A")
            console.console.print(Rule(f"[bold]í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i + 1}/{len(test_cases)} ì‹¤í–‰ (ID: {case_id})[/bold]"))

            initial_state = {
                "case_file": f"ì›ê³  ì£¼ì¥: {case['plaintiff_statement']}\ní”¼ê³  ì£¼ì¥: {case['defendant_statement']}",
                "plaintiff_lawyer": "ì›ê³ ì¸¡ ë³€í˜¸ì‚¬",
                "defendant_lawyer": "í”¼ê³ ì¸¡ ë³€í˜¸ì‚¬",
            }

            final_event = None
            for event in app.stream(initial_state):
                if "__end__" in event:
                    final_event = event["__end__"]

            # LangGraphì˜ ìµœì¢… ì´ë²¤íŠ¸ëŠ” {"state": {...}} í˜¹ì€ {"return": {...}} í˜•íƒœë¡œ
            # ë˜í•‘ë˜ì–´ ì „ë‹¬ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‹¤ì œ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            final_state_data = final_event or {}
            if isinstance(final_state_data, dict):
                if "state" in final_state_data and isinstance(final_state_data["state"], dict):
                    final_state = final_state_data["state"]
                elif "return" in final_state_data and isinstance(final_state_data["return"], dict):
                    final_state = final_state_data["return"]
                else:
                    final_state = final_state_data
            else:
                final_state = {}
            critique_scores = final_state.get("critique_scores", []) or []
            model_outcome = final_state.get("plaintiff_outcome")
            expected_outcome = case.get("expected_outcome")

            row_data: Dict[str, object] = {
                "case_id": case_id,
                "expected_outcome": expected_outcome or "N/A",
                "model_outcome": (model_outcome or ("ë¯¸ì˜ˆì¸¡" if expected_outcome else "N/A")),
                "is_correct": "N/A",
            }

            for criteria, (score_key, reason_key) in CRITERIA_HEADERS.items():
                row_data[score_key] = 0
                row_data[reason_key] = "í‰ê°€ ê²°ê³¼ê°€ ê¸°ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            for item in critique_scores:
                criteria = item.get("criteria")
                if criteria not in CRITERIA_HEADERS:
                    continue
                score_key, reason_key = CRITERIA_HEADERS[criteria]
                score = int(item.get("score", 0))
                reason = item.get("reason") or "í‰ê°€ ì´ìœ ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                row_data[score_key] = score
                row_data[reason_key] = reason
                total_scores[criteria] += score

            if expected_outcome:
                predicted_label = model_outcome or "ë¯¸ì˜ˆì¸¡"
                paired_outcomes.append((expected_outcome, predicted_label))
                if model_outcome:
                    row_data["is_correct"] = "Y" if expected_outcome == model_outcome else "N"
                else:
                    row_data["is_correct"] = "N"
            elif model_outcome:
                row_data["is_correct"] = "ì •ë³´ ë¶€ì¡±"

            writer.writerow([row_data.get(h, "N/A") for h in header])
            total_runs += 1
            time.sleep(1)

    console.print_header(f"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {mode}")
    console.console.print(f"ê²°ê³¼ê°€ [bold cyan]{results_filename}[/bold cyan] íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    table = Table(title="í‰ê·  ì ìˆ˜ (Pass ë¹„ìœ¨)")
    table.add_column("í‰ê°€ í•­ëª©", justify="right", style="cyan", no_wrap=True)
    table.add_column("í‰ê·  ì ìˆ˜ (%)", justify="center", style="magenta")

    if total_runs > 0:
        for criteria in CRITIQUE_CRITERIA:
            avg_score = (total_scores[criteria] / total_runs) * 100 if total_runs else 0.0
            table.add_row(criteria, f"{avg_score:.2f}%")

    console.console.print(table)

    accuracy = 0.0
    macro_f1 = 0.0
    expected_win_rate = 0.0
    model_win_rate = 0.0
    per_label_metrics: List[Tuple[str, float, float, float]] = []

    if paired_outcomes:
        total_cases = len(paired_outcomes)
        correct_predictions = sum(1 for expected, predicted in paired_outcomes if expected == predicted)
        accuracy = correct_predictions / total_cases
        expected_win_rate = sum(1 for expected, _ in paired_outcomes if expected == "ìŠ¹ë¦¬") / total_cases
        model_win_rate = sum(1 for _, predicted in paired_outcomes if predicted == "ìŠ¹ë¦¬") / total_cases
        labels = sorted({expected for expected, _ in paired_outcomes})
        if labels:
            f1_sum = 0.0
            for label in labels:
                tp = sum(1 for expected, predicted in paired_outcomes if expected == label and predicted == label)
                fp = sum(1 for expected, predicted in paired_outcomes if expected != label and predicted == label)
                fn = sum(1 for expected, predicted in paired_outcomes if expected == label and predicted != label)
                precision = tp / (tp + fp) if (tp + fp) else 0.0
                recall = tp / (tp + fn) if (tp + fn) else 0.0
                f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0
                f1_sum += f1
                per_label_metrics.append((label, precision, recall, f1))
            macro_f1 = f1_sum / len(labels) if labels else 0.0

    metrics_table = Table(title="íŒê²° ê²°ê³¼ ì •ëŸ‰ ì§€í‘œ")
    metrics_table.add_column("ì§€í‘œ", justify="left", style="green")
    metrics_table.add_column("ê°’", justify="center", style="white")
    metrics_table.add_row("ì •í™•ë„ (Accuracy)", f"{accuracy * 100:.2f}%")
    metrics_table.add_row("Macro F1", f"{macro_f1:.4f}")
    metrics_table.add_row("ì˜ˆìƒ ì›ê³  ìŠ¹ì†Œìœ¨", f"{expected_win_rate * 100:.2f}%")
    metrics_table.add_row("ëª¨ë¸ ì›ê³  ìŠ¹ì†Œìœ¨", f"{model_win_rate * 100:.2f}%")
    console.console.print(metrics_table)

    if per_label_metrics:
        label_table = Table(title="í´ë˜ìŠ¤ë³„ Precision/Recall/F1")
        label_table.add_column("ë¼ë²¨", style="cyan")
        label_table.add_column("Precision", justify="center")
        label_table.add_column("Recall", justify="center")
        label_table.add_column("F1", justify="center")
        for label, precision, recall, f1 in per_label_metrics:
            label_table.add_row(label, f"{precision:.2f}", f"{recall:.2f}", f"{f1:.2f}")
        console.console.print(label_table)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëª¨ì˜ ë²•ì • ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--mode", type=str, required=True, choices=["trained", "untrained"],
                        help="'trained' ë˜ëŠ” 'untrained' ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    args = parser.parse_args()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    test_dataset_path = os.path.join(current_dir, "data", "test.jsonl")

    if args.mode == "untrained":
        run_benchmark(test_dataset_path, is_trained=False)
    else:
        run_benchmark(test_dataset_path, is_trained=True)
